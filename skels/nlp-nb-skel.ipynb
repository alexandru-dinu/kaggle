{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "991dd280e1aa895006782f2420299f43c365bf3b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.tensor as tensor\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "243278ccda7ad7c9d6d4352f7fca1d44549d1084"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../quora/input\"\n",
    "TRAIN_DATA_FILE = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_DATA_FILE = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}; cols: {list(train_df.columns)}\")\n",
    "print(f\"Test shape: {test_df.shape}; cols: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6390d234bfae617a0628e8d72329f5a212d1edb"
   },
   "outputs": [],
   "source": [
    "sincere = train_df.loc[train_df['target'] == 0]\n",
    "insincere = train_df.loc[train_df['target'] == 1]\n",
    "\n",
    "print(\n",
    "    f\"sincere: {len(sincere)} ({round(100.0 * len(sincere)/len(train_df), 3)}%); \"\n",
    "    f\"insincere: {len(insincere)} ({round(100.0 * len(insincere)/len(train_df), 3)}%); \"\n",
    "    f\"ratio (-/+): {round(len(sincere)/len(insincere), 3)}; \"\n",
    "    f\"ratio (+/-): {round(len(insincere)/len(sincere), 3)}\\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"sincere: {sincere.iloc[random.randint(0, len(sincere))]['question_text']}\\n\\n\"\n",
    "    f\"insincere: {insincere.iloc[random.randint(0, len(insincere))]['question_text']}\"\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_GLOVE = f\"{DATA_DIR}/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "EMB_WORD2VEC = f\"{DATA_DIR}/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\n",
    "EMB_PARAGRAM = f\"{DATA_DIR}/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "EMB_WIKI = f\"{DATA_DIR}/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_word2vec = KeyedVectors.load_word2vec_format(EMB_WORD2VEC, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(emb_word2vec.vocab)} x {emb_word2vec['the'].size}\")\n",
    "print(\"xiaomi\" in emb_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_GLOVE, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "\n",
    "emb_glove = load_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(emb_glove)} x {emb_glove['a'].size}\")\n",
    "print(\"xiaomi\" in emb_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e6ac0681544ffa4ddf6af342222d80f9407fda3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PUNCTUATION = {\n",
    "    'sep'   : u'\\u200b' + \"/-'´‘…—−–\",\n",
    "    'keep'  : \"&\",\n",
    "    'remove': '?!.,，\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~“”’™•°'\n",
    "}\n",
    "\n",
    "GLOVE_SYN_DICT = {\n",
    "    'cryptocurrencies': 'crypto currencies',\n",
    "    'ethereum'        : 'crypto currency',\n",
    "    'fortnite'        : 'video game',\n",
    "    'quorans'         : 'quora members',\n",
    "    'brexit'          : 'britain exit',\n",
    "    'redmi'           : 'xiaomi',\n",
    "    '√'               : 'square root',\n",
    "    '÷'               : 'division',\n",
    "    '∞'               : 'infinity',\n",
    "    '€'               : 'euro',\n",
    "    '£'               : 'pound sterling',\n",
    "    '$'               : 'dollar',\n",
    "    '₹'               : 'rupee',\n",
    "    '×'               : 'product',\n",
    "    'ã'               : 'a',\n",
    "    'è'               : 'e',\n",
    "    'é'               : 'e',\n",
    "    'ö'               : 'o',\n",
    "    '²'               : 'squared',\n",
    "    '∈'               : 'in',\n",
    "    '∩'               : 'intersection',\n",
    "    u'\\u0398'         : 'Theta',\n",
    "    u'\\u03A0'         : 'Pi',\n",
    "    u'\\u03A9'         : 'Omega',\n",
    "    u'\\u0392'         : 'Beta',\n",
    "    u'\\u03B8'         : 'theta',\n",
    "    u'\\u03C0'         : 'pi',\n",
    "    u'\\u03C9'         : 'omega',\n",
    "    u'\\u03B2'         : 'beta',\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return list(map(lambda w: w.strip(), s.split()))\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "\n",
    "    for p in PUNCTUATION['sep']:\n",
    "        x = x.replace(p, \" \")\n",
    "    for p in PUNCTUATION['keep']:\n",
    "        x = x.replace(p, f\" {p} \")\n",
    "    for p in PUNCTUATION['remove']:\n",
    "        x = x.replace(p, \"\")\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_syn(x):\n",
    "    regex = re.compile('(%s)' % '|'.join(GLOVE_SYN_DICT.keys()))\n",
    "    return regex.sub(lambda m: GLOVE_SYN_DICT.get(m.group(0), ''), x)\n",
    "\n",
    "\n",
    "def clean_all(x):\n",
    "    x = clean_text(x)\n",
    "    x = clean_syn(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "def build_vocabulary(df: pd.DataFrame) -> Counter:\n",
    "    sentences = df.progress_apply(tokenize).values\n",
    "    vocab = Counter()\n",
    "    s_len = []\n",
    "    \n",
    "    for sentence in tqdm(sentences):  \n",
    "        s_len.append(len(sentence))\n",
    "        for word in sentence:\n",
    "            vocab[word] += 1\n",
    "    return vocab, np.array(s_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "train_df[\"clean_question_text\"] = train_df[\"question_text\"].progress_apply(clean_all)\n",
    "test_df[\"clean_question_text\"] = test_df[\"question_text\"].progress_apply(clean_all)\n",
    "\n",
    "# vocab\n",
    "train_vocab, train_s_len = build_vocabulary(train_df[\"clean_question_text\"])\n",
    "test_vocab, test_s_len = build_vocabulary(test_df[\"clean_question_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = scipy.stats.describe(train_s_len)\n",
    "d_test = scipy.stats.describe(test_s_len)\n",
    "print(f\"train: {d_train}, median: {np.median(train_s_len)}\")\n",
    "print(f\"test: {d_test}, median: {np.median(test_s_len)}\")\n",
    "\n",
    "nb = 60\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(train_s_len, bins=nb, range=[0, 60], facecolor='red', label='train')\n",
    "\n",
    "plt.hist(test_s_len, bins=nb, range=[0, 60], facecolor='blue', label='test')\n",
    "plt.axvline(x=d_test.mean, color='cyan')\n",
    "\n",
    "plt.title(\"Sentence length\", size=24)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 16})\n",
    "plt.xticks([5*i for i in range(14)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_n = 20\n",
    "exclude = [\n",
    "    \"the\", \"of\", \"and\", \"to\", \"a\", \"in\", \"is\", \"i\",\n",
    "    \"that\", \"it\", \"for\", \"you\", \"was\", \"with\", \"on\",\n",
    "    \"as\", \"have\", \"but\", \"be\", \"they\"\n",
    "]\n",
    "\n",
    "for w in exclude:\n",
    "    del train_vocab[w]\n",
    "    del test_vocab[w]\n",
    "    \n",
    "Tmc = train_vocab.most_common()\n",
    "tmc = test_vocab.most_common()\n",
    "\n",
    "for i in range(_n):\n",
    "    print(f\"{Tmc[i]} -- {tmc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X-not-in-Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_in_train = Counter()\n",
    "train_not_in_test = Counter()\n",
    "\n",
    "for w in test_vocab:\n",
    "    if w not in train_vocab:\n",
    "        test_not_in_train[w] += 1\n",
    "\n",
    "for w in train_vocab:\n",
    "    if w not in test_vocab:\n",
    "        train_not_in_test[w] += 1\n",
    "        \n",
    "train_uniq_words = set(train_vocab.keys())\n",
    "test_uniq_words = set(test_vocab.keys())\n",
    "uniq_words = set(train_uniq_words.union(test_uniq_words))\n",
    "all_oov = Counter()\n",
    "\n",
    "for w in uniq_words:\n",
    "    if w not in emb_glove:\n",
    "        all_oov[w] += 1\n",
    "        \n",
    "print(f\"train not in test: {len(train_not_in_test)}\")\n",
    "print(f\"test not in train: {len(test_not_in_train)}\")\n",
    "print(f\"train uniq: {len(train_uniq_words)}\")\n",
    "print(f\"test uniq: {len(test_uniq_words)}\")\n",
    "print(f\"total uniq words: {len(uniq_words)}\")\n",
    "\n",
    "# all_oov.most_common(10)\n",
    "test_not_in_train.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_vocab_coverage(vocab, emb) -> (Counter, Counter):\n",
    "    oov = Counter() # out-of-vocab\n",
    "    inv = Counter() # in-vocab\n",
    "    oov_uniq_num = inv_uniq_num = 0.0\n",
    "    oov_all_num = inv_all_num = 0.0\n",
    "    \n",
    "    for w in tqdm(vocab):\n",
    "        if w in emb:\n",
    "            inv[w] = vocab[w]\n",
    "            inv_uniq_num += 1\n",
    "            inv_all_num += vocab[w]\n",
    "        else:\n",
    "            oov[w] = vocab[w]\n",
    "            oov_uniq_num += 1\n",
    "            oov_all_num += vocab[w]\n",
    "    \n",
    "    cov_uniq = 100.0 * round(inv_uniq_num / len(vocab), 5)\n",
    "    cov_all = 100.0 * round(inv_all_num / (inv_all_num + oov_all_num), 5)\n",
    "    \n",
    "    print(f\"oov_uniq: {oov_uniq_num}; inv_uniq: {inv_uniq_num}; all_uniq: {len(vocab)}\")\n",
    "    print(\"embeddings-vocabulary coverage (unique): %.3f%%\" % cov_uniq)\n",
    "    print(\"embeddings-vocabulary coverage (all text): %.3f%%\" % cov_all)\n",
    "    \n",
    "    return oov, inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov, inv = get_emb_vocab_coverage(train_vocab, emb_glove)\n",
    "oov.most_common(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov, inv = get_emb_vocab_coverage(test_vocab, emb_glove)\n",
    "oov.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # GloVe emb matrix\n",
    "        num_words, emb_size = emb_matrix.shape\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(num_words, emb_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=2*self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(2 * self.hidden_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B x sen_maxlen\n",
    "\n",
    "        emb = self.embedding(x)\n",
    "        # B x sen_maxlen x emb_size\n",
    "\n",
    "        out_lstm, _ = self.lstm(emb)\n",
    "        # B x sen_maxlen x (2*sen_maxlen)\n",
    "        \n",
    "        _, h_gru = self.gru(self.dropout(out_lstm))\n",
    "        # 2 x B x sen_maxlen\n",
    "        \n",
    "        h_gru = h_gru.permute((1, 0, 2)).reshape(x.size(0), -1)\n",
    "        # B x (2*sen_maxlen)\n",
    "        \n",
    "        out = self.fc(h_gru).unsqueeze(0)\n",
    "        # 1 x B x 1\n",
    "        \n",
    "        return out\n",
    "\n",
    "# --\n",
    "bs = 8\n",
    "x = torch.zeros((bs, 70), dtype=torch.long)\n",
    "m = Net(emb_matrix=np.load(\"glove_embedding_matrix.npy\"), hidden_size=70)\n",
    "\n",
    "y = m(x)\n",
    "\n",
    "y.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
