{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "991dd280e1aa895006782f2420299f43c365bf3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from functools import reduce\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.tensor as tensor\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "243278ccda7ad7c9d6d4352f7fca1d44549d1084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1306122, 3); cols: ['qid', 'question_text', 'target']\n",
      "Test shape: (375806, 2); cols: ['qid', 'question_text']\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../input\"\n",
    "TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_CSV = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}; cols: {list(train_df.columns)}\")\n",
    "print(f\"Test shape: {test_df.shape}; cols: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "b6390d234bfae617a0628e8d72329f5a212d1edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sincere: 1225312 (93.813%); insincere: 80810 (6.187%); ratio (-/+): 15.163; ratio (+/-): 0.066\n",
      "\n",
      "sincere: Is Islam a good thing?\n",
      "\n",
      "insincere: What is the point of letting criminals that committed felonies back to the society? Why can't we simply lock them up like nuclear waste?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sincere = train_df.loc[train_df['target'] == 0]\n",
    "insincere = train_df.loc[train_df['target'] == 1]\n",
    "\n",
    "print(\n",
    "    f\"sincere: {len(sincere)} ({round(100.0 * len(sincere)/len(train_df), 3)}%); \"\n",
    "    f\"insincere: {len(insincere)} ({round(100.0 * len(insincere)/len(train_df), 3)}%); \"\n",
    "    f\"ratio (-/+): {round(len(sincere)/len(insincere), 3)}; \"\n",
    "    f\"ratio (+/-): {round(len(insincere)/len(sincere), 3)}\\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"sincere: {sincere.iloc[random.randint(0, len(sincere))]['question_text']}\\n\\n\"\n",
    "    f\"insincere: {insincere.iloc[random.randint(0, len(insincere))]['question_text']}\"\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_GLOVE_FILE = f\"{DATA_DIR}/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "EMB_WORD2VEC_FILE = f\"{DATA_DIR}/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\n",
    "EMB_PARAGRAM_FILE = f\"{DATA_DIR}/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "EMB_WIKI_FILE = f\"{DATA_DIR}/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_word2vec = KeyedVectors.load_word2vec_format(EMB_WORD2VEC_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emb_word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6f796f4415b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{len(emb_word2vec.vocab)} x {emb_word2vec['the'].size}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xiaomi\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memb_word2vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emb_word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"{len(emb_word2vec.vocab)} x {emb_word2vec['the'].size}\")\n",
    "print(\"xiaomi\" in emb_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_WIKI_FILE) if len(o)>100)\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "emb_wiki = load_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999994 x 300\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(emb_wiki)} x {emb_wiki['the'].size}\")\n",
    "print(\"xiaomi\" in emb_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_GLOVE_FILE, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "\n",
    "emb_glove = load_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196016 x 300\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(emb_glove)} x {emb_glove['a'].size}\")\n",
    "print(\"xiaomi\" in emb_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParaGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paragram():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_PARAGRAM_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    return embeddings_index\n",
    "    \n",
    "emb_paragram = load_paragram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1703755 x 300\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(emb_paragram)} x {emb_paragram['the'].size}\")\n",
    "print(\"paytm\" in emb_paragram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1253d44aae34b978cdbe694121acd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a712a547234bdcbe837df3637a3f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2196016), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef886992cb84022928db38e5ef33443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1703755), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def combine_emb_dicts(*embs):\n",
    "    out_emb = defaultdict(lambda : np.zeros(300, dtype=np.float32))\n",
    "    \n",
    "    n = len(embs)\n",
    "    \n",
    "    for emb in tqdm(embs, total=n):\n",
    "        for w, e in tqdm(emb.items()):\n",
    "            out_emb[w] += (1.0/n) * e\n",
    "                \n",
    "    return out_emb\n",
    "            \n",
    "\n",
    "emb_glove_paragram = combine_emb_dicts(emb_glove, emb_paragram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emb_word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-298449d847f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0m_w2v_not_glove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_word2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memb_glove\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0m_w2v_not_glove\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emb_word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "_glove_not_w2v = Counter()\n",
    "_w2v_not_glove = Counter()\n",
    "\n",
    "for w in tqdm(emb_word2vec.vocab):\n",
    "    if w not in emb_glove:\n",
    "        _w2v_not_glove[w] += 1\n",
    "\n",
    "for w in tqdm(emb_glove):\n",
    "    if w not in emb_word2vec:\n",
    "        _glove_not_w2v[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"glove not w2v: {len(_glove_not_w2v)}; w2v not glove: {len(_w2v_not_glove)}\")\n",
    "print(\"-\" * 64)\n",
    "print(random.sample(set(_w2v_not_glove), 10))\n",
    "print(\"-\" * 64)\n",
    "print(random.sample(set(_glove_not_w2v), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "0e6ac0681544ffa4ddf6af342222d80f9407fda3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PUNCTUATION = {\n",
    "    'sep'   : u'\\u200b' + \"/-'´′‘…—−–\",\n",
    "    'keep'  : \"&\",\n",
    "    'remove': '?!.,，\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~“”’™•°'\n",
    "}\n",
    "\n",
    "SYN_DICT = {\n",
    "    'cryptocurrencies': 'crypto currencies',\n",
    "    'ethereum'        : 'crypto currency',\n",
    "    'coinbase'        : 'crypto platform',\n",
    "    'altcoin'         : 'crypto currency',\n",
    "    'altcoins'        : 'crypto currency',\n",
    "    'litecoin'        : 'crypto currency',\n",
    "    'fortnite'        : 'video game',\n",
    "    'quorans'         : 'quora members',\n",
    "    'quoras'          : 'quora members',\n",
    "    'qoura'           : 'quora',\n",
    "    'brexit'          : 'britain exit',\n",
    "    'redmi'           : 'phone',\n",
    "    'oneplus'         : 'phone',\n",
    "    'hackerrank'      : 'programming challenges',\n",
    "    'bhakts'          : 'gullible',\n",
    "    '√'               : 'square root',\n",
    "    '÷'               : 'division',\n",
    "    '∞'               : 'infinity',\n",
    "    '€'               : 'euro',\n",
    "    '£'               : 'pound sterling',\n",
    "    '$'               : 'dollar',\n",
    "    '₹'               : 'rupee',\n",
    "    '×'               : 'product',\n",
    "    'ã'               : 'a',\n",
    "    'è'               : 'e',\n",
    "    'é'               : 'e',\n",
    "    'ö'               : 'o',\n",
    "    '²'               : 'squared',\n",
    "    '∈'               : 'in',\n",
    "    '∩'               : 'intersection',\n",
    "    u'\\u0398'         : 'Theta',\n",
    "    u'\\u03A0'         : 'Pi',\n",
    "    u'\\u03A9'         : 'Omega',\n",
    "    u'\\u0392'         : 'Beta',\n",
    "    u'\\u03B8'         : 'theta',\n",
    "    u'\\u03C0'         : 'pi',\n",
    "    u'\\u03C9'         : 'omega',\n",
    "    u'\\u03B2'         : 'beta',\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return list(map(lambda w: w.strip(), s.split()))\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "\n",
    "    for p in PUNCTUATION['sep']:\n",
    "        x = x.replace(p, \" \")\n",
    "    for p in PUNCTUATION['keep']:\n",
    "        x = x.replace(p, f\" {p} \")\n",
    "    for p in PUNCTUATION['remove']:\n",
    "        x = x.replace(p, \"\")\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_site(x):\n",
    "    regex = re.compile('(www)([a-z0-9]+)(com|org)')\n",
    "    return regex.sub(lambda m: m.group(2), x)\n",
    "\n",
    "\n",
    "def clean_syn(x):\n",
    "    regex = re.compile('(%s)' % '|'.join(SYN_DICT.keys()))\n",
    "    return regex.sub(lambda m: SYN_DICT.get(m.group(0), ''), x)\n",
    "\n",
    "\n",
    "def clean_all(x):\n",
    "    x = clean_text(x)\n",
    "    x = clean_syn(x)\n",
    "    x = clean_site(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def build_vocabulary(df: pd.DataFrame) -> Counter:\n",
    "    sentences = df.progress_apply(tokenize).values\n",
    "    vocab = Counter()\n",
    "    s_len = []\n",
    "    \n",
    "    for sentence in tqdm(sentences):  \n",
    "        s_len.append(len(sentence))\n",
    "        for word in sentence:\n",
    "            vocab[word] += 1\n",
    "    return vocab, np.array(s_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b024679d72402c9becd153da5fcf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5163a24d8794edcbc553d199acd2ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=375806), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97b826c5a8a4da89c51ebe28f46c25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5985918f1fb488d8c3336231565f9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd34d443c614a15a7ab8b0ed8631902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=375806), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ae9941fa154c0d89f80025a31c86e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=375806), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clean\n",
    "train_df[\"clean_question_text\"] = train_df[\"question_text\"].progress_apply(clean_all)\n",
    "test_df[\"clean_question_text\"] = test_df[\"question_text\"].progress_apply(clean_all)\n",
    "\n",
    "# vocab\n",
    "train_vocab, train_s_len = build_vocabulary(train_df[\"clean_question_text\"])\n",
    "test_vocab, test_s_len = build_vocabulary(test_df[\"clean_question_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = scipy.stats.describe(train_s_len)\n",
    "d_test = scipy.stats.describe(test_s_len)\n",
    "print(f\"train: {d_train}, median: {np.median(train_s_len)}\")\n",
    "print(f\"test: {d_test}, median: {np.median(test_s_len)}\")\n",
    "\n",
    "nb = 60\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(train_s_len, bins=nb, range=[0, 60], facecolor='red', label='train')\n",
    "\n",
    "plt.hist(test_s_len, bins=nb, range=[0, 60], facecolor='blue', label='test')\n",
    "plt.axvline(x=d_test.mean, color='cyan')\n",
    "\n",
    "plt.title(\"Sentence length\", size=24)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 16})\n",
    "plt.xticks([5*i for i in range(14)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_n = 20\n",
    "exclude = [\n",
    "    \"the\", \"of\", \"and\", \"to\", \"a\", \"in\", \"is\", \"i\",\n",
    "    \"that\", \"it\", \"for\", \"you\", \"was\", \"with\", \"on\",\n",
    "    \"as\", \"have\", \"but\", \"be\", \"they\"\n",
    "]\n",
    "\n",
    "for w in exclude:\n",
    "    del train_vocab[w]\n",
    "    del test_vocab[w]\n",
    "    \n",
    "Tmc = train_vocab.most_common()\n",
    "tmc = test_vocab.most_common()\n",
    "\n",
    "for i in range(_n):\n",
    "    print(f\"{Tmc[i]} -- {tmc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Less common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "Tmc = train_vocab.most_common()[:-n-1:-1]\n",
    "tmc = test_vocab.most_common()[:-n-1:-1]\n",
    "\n",
    "u = 0\n",
    "t = 10\n",
    "for w in train_vocab:\n",
    "    u += (train_vocab[w] <= t)\n",
    "print(f\"[train] {round(100.0 * u/len(train_vocab), 3)}% words have <= {t} occurences\")\n",
    "    \n",
    "u = 0\n",
    "t = 10\n",
    "for w in test_vocab:\n",
    "    u += (test_vocab[w] <= t)\n",
    "print(f\"[test]  {round(100.0 * u/len(train_vocab), 3)}% words have <= {t} occurences\")\n",
    "\n",
    "print()\n",
    "    \n",
    "for i in range(n):\n",
    "    print(f\"{Tmc[i]} -- {tmc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_in_train = Counter()\n",
    "train_not_in_test = Counter()\n",
    "\n",
    "for w in test_vocab:\n",
    "    if w not in train_vocab:\n",
    "        test_not_in_train[w] += 1\n",
    "\n",
    "for w in train_vocab:\n",
    "    if w not in test_vocab:\n",
    "        train_not_in_test[w] += 1\n",
    "        \n",
    "train_uniq_words = set(train_vocab.keys())\n",
    "test_uniq_words = set(test_vocab.keys())\n",
    "uniq_words = set(train_uniq_words.union(test_uniq_words))\n",
    "all_oov = Counter()\n",
    "\n",
    "for w in uniq_words:\n",
    "    if w not in emb_glove:\n",
    "        all_oov[w] += 1\n",
    "        \n",
    "print(f\"train not in test: {len(train_not_in_test)}\")\n",
    "print(f\"test not in train: {len(test_not_in_train)}\")\n",
    "print(f\"train uniq: {len(train_uniq_words)}\")\n",
    "print(f\"test uniq: {len(test_uniq_words)}\")\n",
    "print(f\"total uniq words: {len(uniq_words)}\")\n",
    "\n",
    "# all_oov.most_common(10)\n",
    "\",\".join([x for (x, _) in test_not_in_train.most_common(50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandleMisspellings:\n",
    "\n",
    "    def __init__(self, all_words_set, words2idx):\n",
    "        self.all_words_set = all_words_set\n",
    "        self.words2idx = words2idx\n",
    "\n",
    "    def prob(self, word):\n",
    "        return self.words2idx.get(word, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def one_edit(word):\n",
    "        letters = string.ascii_lowercase\n",
    "\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def known(self, words):\n",
    "        return set(words).intersection(self.all_words_set)\n",
    "\n",
    "    def candidates(self, word):\n",
    "        return self.known([word]).union(self.known(self.one_edit(word)))\n",
    "\n",
    "    def correct(self, word):\n",
    "        cs = self.candidates(word)\n",
    "        return word if len(cs) == 0 else min(cs, key=lambda w: self.prob(w))\n",
    "\n",
    "misspelling_handler = HandleMisspellings(\n",
    "    all_words_set=set(list(emb_glove_paragram.keys())),\n",
    "    words2idx={w: i for (i, w) in enumerate(emb_glove_paragram.keys())}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'danger'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = time.time()\n",
    "t = 0\n",
    "misspelling_handler.correct('dang3r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embbedding coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_vocab_coverage(vocab, emb) -> (Counter, Counter):\n",
    "    oov = Counter() # out-of-vocab\n",
    "    inv = Counter() # in-vocab\n",
    "    oov_uniq_num = inv_uniq_num = 0.0\n",
    "    oov_all_num = inv_all_num = 0.0\n",
    "    \n",
    "    for w in tqdm(vocab):\n",
    "        if w in emb or misspelling_handler.correction(w) in emb:\n",
    "            inv[w] = vocab[w]\n",
    "            inv_uniq_num += 1\n",
    "            inv_all_num += vocab[w]\n",
    "        else:\n",
    "            oov[w] = vocab[w]\n",
    "            oov_uniq_num += 1\n",
    "            oov_all_num += vocab[w]\n",
    "    \n",
    "    cov_uniq = 100.0 * round(inv_uniq_num / len(vocab), 5)\n",
    "    cov_all = 100.0 * round(inv_all_num / (inv_all_num + oov_all_num), 5)\n",
    "    \n",
    "    print(f\"oov_uniq: {oov_uniq_num}; inv_uniq: {inv_uniq_num}; vocab_size: {len(vocab)}\")\n",
    "    print(\"embeddings-vocabulary coverage (unique): %.3f%%\" % cov_uniq)\n",
    "    print(\"embeddings-vocabulary coverage (all text): %.3f%%\" % cov_all)\n",
    "    \n",
    "    return oov, inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6955e8a16b4f8fb4a8efd95345fe5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=207016), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "oov_uniq: 25281.0; inv_uniq: 181735.0; vocab_size: 207016\n",
      "embeddings-vocabulary coverage (unique): 87.788%\n",
      "embeddings-vocabulary coverage (all text): 99.814%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'adityanath(106),alshamsi(92),unacademy(86),zerodha(80),tensorflow(73),doklam(70),lnmiit(68),kavalireddi(58),sgsits(40),nanodegree(38),gurugram(38),mhtcet(38),microservices(36),clickbait(33),lbsnaa(33),chromecast(31),naukricom(31),quoracom(29),demonitisation(29),bookingcom(29),bitconnect(28),deepmind(28),jungkook(28),trumpcare(27),wannacry(26),xxxtentacion(26),freelancercom(25),onedrive(25),codeforces(24),arrowverse(24),electroneum(24),sterling1000(23),genderfluid(23),internshala(22),chapterwise(22),igdtuw(22),ravindrababu(22),twinflame(22),padmaavat(21),tissnet(21),undergraduation(21),wordpresscom(21),hackerearth(21),veerwal(20),wikitribune(19),dhinchak(19),pizzagate(18),theranos(18),covfefe(18),yourquote(17)'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov, inv = get_emb_vocab_coverage(train_vocab, emb_glove_paragram)\n",
    "\",\".join([x + f\"({y})\" for (x, y) in oov.most_common(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9db18db43fc47658a6683b17cbbd70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=105781), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "oov_uniq: 7904.0; inv_uniq: 97877.0; vocab_size: 105781\n",
      "embeddings-vocabulary coverage (unique): 92.528%\n",
      "embeddings-vocabulary coverage (all text): 99.815%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'alshamsi(31),tensorflow(29),unacademy(27),adityanath(26),lnmiit(23),zerodha(20),chromecast(19),kavalireddi(16),lbsnaa(14),gurugram(11)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov, inv = get_emb_vocab_coverage(test_vocab, emb_paragram)\n",
    "\",\".join([x + f\"({y})\" for (x, y) in oov.most_common(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emb_wiki' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a403a371af83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moov_thrd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moov_thrd\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memb_wiki\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-a403a371af83>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moov_thrd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moov_thrd\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memb_wiki\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'emb_wiki' is not defined"
     ]
    }
   ],
   "source": [
    "oov_thrd = [x for (x, y) in oov.most_common() if y > 0]\n",
    "len([w for w in oov_thrd if w in emb_wiki])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, with_bias=False):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.with_bias = with_bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight, requires_grad=True)\n",
    "        \n",
    "        if with_bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.with_bias:\n",
    "            eij += self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "\n",
    "        a /= torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        \n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        num_words, emb_size = emb_matrix.shape\n",
    "\n",
    "        # sentence maxlen\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(num_words, emb_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.bidir_lstm = nn.LSTM(\n",
    "            input_size=emb_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.lstm_attention = Attention(\n",
    "            feature_dim=2 * self.hidden_size, step_dim=self.hidden_size, with_bias=False\n",
    "        )\n",
    "\n",
    "        self.bidir_gru = nn.GRU(\n",
    "            input_size=2 * self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.gru_attention = Attention(\n",
    "            feature_dim=2 * self.hidden_size, step_dim=self.hidden_size, with_bias=False\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(4 * 2 * self.hidden_size, 2 * self.hidden_size)\n",
    "        self.fc2 = nn.Linear(2 * self.hidden_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B x sen_maxlen\n",
    "\n",
    "        emb = self.embedding(x)\n",
    "        # B x sen_maxlen x emb_size\n",
    "\n",
    "        out_lstm, _ = self.bidir_lstm(emb)\n",
    "        # B x sen_maxlen x (2*sen_maxlen)\n",
    "\n",
    "        out_lstm_atn = self.lstm_attention(out_lstm)\n",
    "        # B x (2*sen_maxlen)\n",
    "\n",
    "        out_gru, _ = self.bidir_gru(self.dropout(out_lstm))\n",
    "        # B x sen_maxlen x (2*sen_maxlen)\n",
    "\n",
    "        out_gru_atn = self.gru_attention(out_gru)\n",
    "        # B x (2*sen_maxlen)\n",
    "\n",
    "        # pooling\n",
    "        avg_pool = torch.mean(out_gru, dim=1)\n",
    "        # B x (2*sen_maxlen)\n",
    "        max_pool, _ = torch.max(out_gru, dim=1)\n",
    "        # B x (2*sen_maxlen)\n",
    "\n",
    "        # concatenate results\n",
    "        out = torch.cat((out_lstm_atn, out_gru_atn, avg_pool, max_pool), dim=1)\n",
    "        # B x (4 * 2*sen_maxlen)\n",
    "\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.fc2(self.dropout(out)).unsqueeze(0)\n",
    "        # 1 x B x 1\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 8\n",
    "x = torch.zeros((bs, 70), dtype=torch.long)\n",
    "m = Net(emb_matrix=np.zeros((1000,300)), hidden_size=70)\n",
    "\n",
    "y = m(x)\n",
    "\n",
    "y.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
