{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "991dd280e1aa895006782f2420299f43c365bf3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from functools import reduce\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.tensor as tensor\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "243278ccda7ad7c9d6d4352f7fca1d44549d1084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1306122, 3); cols: ['qid', 'question_text', 'target']\n",
      "Test shape: (375806, 2); cols: ['qid', 'question_text']\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../input\"\n",
    "TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_CSV = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}; cols: {list(train_df.columns)}\")\n",
    "print(f\"Test shape: {test_df.shape}; cols: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b6390d234bfae617a0628e8d72329f5a212d1edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sincere: 1225312 (93.813%); insincere: 80810 (6.187%); ratio (-/+): 15.163; ratio (+/-): 0.066\n",
      "\n",
      "sincere: What gemstone are you?\n",
      "\n",
      "insincere: Why Indians always say their ancestors have already invented all technological innovations and site their religious books as reference?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sincere = train_df.loc[train_df['target'] == 0]\n",
    "insincere = train_df.loc[train_df['target'] == 1]\n",
    "\n",
    "print(\n",
    "    f\"sincere: {len(sincere)} ({round(100.0 * len(sincere)/len(train_df), 3)}%); \"\n",
    "    f\"insincere: {len(insincere)} ({round(100.0 * len(insincere)/len(train_df), 3)}%); \"\n",
    "    f\"ratio (-/+): {round(len(sincere)/len(insincere), 3)}; \"\n",
    "    f\"ratio (+/-): {round(len(insincere)/len(sincere), 3)}\\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"sincere: {sincere.iloc[random.randint(0, len(sincere))]['question_text']}\\n\\n\"\n",
    "    f\"insincere: {insincere.iloc[random.randint(0, len(insincere))]['question_text']}\"\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_GLOVE_FILE = f\"{DATA_DIR}/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "EMB_WORD2VEC_FILE = f\"{DATA_DIR}/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\n",
    "EMB_PARAGRAM_FILE = f\"{DATA_DIR}/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "EMB_WIKI_FILE = f\"{DATA_DIR}/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_word2vec = KeyedVectors.load_word2vec_format(EMB_WORD2VEC_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(emb_word2vec.vocab)} x {emb_word2vec['the'].size}\")\n",
    "print(\"xiaomi\" in emb_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_WIKI_FILE) if len(o)>100)\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "emb_wiki = load_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(emb_wiki)} x {emb_wiki['the'].size}\")\n",
    "print(\"xiaomi\" in emb_wiki)\n",
    "      \n",
    "random.sample(emb_wiki.keys(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_GLOVE_FILE, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "\n",
    "emb_glove = load_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196016 x 300\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(emb_glove)} x {emb_glove['a'].size}\")\n",
    "print(\"xiaomi\" in emb_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParaGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paragram():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_PARAGRAM_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    return embeddings_index\n",
    "    \n",
    "emb_paragram = load_paragram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1703755 x 300\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(emb_paragram)} x {emb_paragram['the'].size}\")\n",
    "print(\"paytm\" in emb_paragram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emb_word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-298449d847f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0m_w2v_not_glove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_word2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memb_glove\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0m_w2v_not_glove\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emb_word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "_glove_not_w2v = Counter()\n",
    "_w2v_not_glove = Counter()\n",
    "\n",
    "for w in tqdm(emb_word2vec.vocab):\n",
    "    if w not in emb_glove:\n",
    "        _w2v_not_glove[w] += 1\n",
    "\n",
    "for w in tqdm(emb_glove):\n",
    "    if w not in emb_word2vec:\n",
    "        _glove_not_w2v[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"glove not w2v: {len(_glove_not_w2v)}; w2v not glove: {len(_w2v_not_glove)}\")\n",
    "print(\"-\" * 64)\n",
    "print(random.sample(set(_w2v_not_glove), 10))\n",
    "print(\"-\" * 64)\n",
    "print(random.sample(set(_glove_not_w2v), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "0e6ac0681544ffa4ddf6af342222d80f9407fda3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PUNCTUATION = {\n",
    "    'sep'   : u'\\u200b' + \"/-'´′‘…—−–\",\n",
    "    'keep'  : \"&\",\n",
    "    'remove': '?!.,，\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~“”’™•°'\n",
    "}\n",
    "\n",
    "SYN_DICT = {\n",
    "    'cryptocurrencies': 'crypto currencies',\n",
    "    'ethereum'        : 'crypto currency',\n",
    "    'coinbase'        : 'crypto platform',\n",
    "    'altcoin'         : 'crypto currency',\n",
    "    'altcoins'        : 'crypto currency',\n",
    "    'litecoin'        : 'crypto currency',\n",
    "    'fortnite'        : 'video game',\n",
    "    'quorans'         : 'quora members',\n",
    "    'quoras'          : 'quora members',\n",
    "    'qoura'           : 'quora',\n",
    "    'brexit'          : 'britain exit',\n",
    "    'redmi'           : 'phone',\n",
    "    'oneplus'         : 'phone',\n",
    "    'hackerrank'      : 'programming challenges',\n",
    "    'bhakts'          : 'gullible',\n",
    "    '√'               : 'square root',\n",
    "    '÷'               : 'division',\n",
    "    '∞'               : 'infinity',\n",
    "    '€'               : 'euro',\n",
    "    '£'               : 'pound sterling',\n",
    "    '$'               : 'dollar',\n",
    "    '₹'               : 'rupee',\n",
    "    '×'               : 'product',\n",
    "    'ã'               : 'a',\n",
    "    'è'               : 'e',\n",
    "    'é'               : 'e',\n",
    "    'ö'               : 'o',\n",
    "    '²'               : 'squared',\n",
    "    '∈'               : 'in',\n",
    "    '∩'               : 'intersection',\n",
    "    u'\\u0398'         : 'Theta',\n",
    "    u'\\u03A0'         : 'Pi',\n",
    "    u'\\u03A9'         : 'Omega',\n",
    "    u'\\u0392'         : 'Beta',\n",
    "    u'\\u03B8'         : 'theta',\n",
    "    u'\\u03C0'         : 'pi',\n",
    "    u'\\u03C9'         : 'omega',\n",
    "    u'\\u03B2'         : 'beta',\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return list(map(lambda w: w.strip(), s.split()))\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "\n",
    "    for p in PUNCTUATION['sep']:\n",
    "        x = x.replace(p, \" \")\n",
    "    for p in PUNCTUATION['keep']:\n",
    "        x = x.replace(p, f\" {p} \")\n",
    "    for p in PUNCTUATION['remove']:\n",
    "        x = x.replace(p, \"\")\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_syn(x):\n",
    "    regex = re.compile('(%s)' % '|'.join(SYN_DICT.keys()))\n",
    "    return regex.sub(lambda m: SYN_DICT.get(m.group(0), ''), x)\n",
    "\n",
    "\n",
    "def clean_all(x):\n",
    "    x = clean_text(x)\n",
    "    x = clean_syn(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "def build_vocabulary(df: pd.DataFrame) -> Counter:\n",
    "    sentences = df.progress_apply(tokenize).values\n",
    "    vocab = Counter()\n",
    "    s_len = []\n",
    "    \n",
    "    for sentence in tqdm(sentences):  \n",
    "        s_len.append(len(sentence))\n",
    "        for word in sentence:\n",
    "            vocab[word] += 1\n",
    "    return vocab, np.array(s_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b902490c1a478f891417007fc479cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6676ac8d308a4747b0410a2229a279c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=375806), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b914457a8c54e55b89939207bd4be4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09c6730639b42cca06c9d57ff75fdc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c5e56907ce4d68b1b5a32d3ff90be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=375806), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06cb7ab04124419bf6046864eb11651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=375806), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clean\n",
    "train_df[\"clean_question_text\"] = train_df[\"question_text\"].progress_apply(clean_all)\n",
    "test_df[\"clean_question_text\"] = test_df[\"question_text\"].progress_apply(clean_all)\n",
    "\n",
    "# vocab\n",
    "train_vocab, train_s_len = build_vocabulary(train_df[\"clean_question_text\"])\n",
    "test_vocab, test_s_len = build_vocabulary(test_df[\"clean_question_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = scipy.stats.describe(train_s_len)\n",
    "d_test = scipy.stats.describe(test_s_len)\n",
    "print(f\"train: {d_train}, median: {np.median(train_s_len)}\")\n",
    "print(f\"test: {d_test}, median: {np.median(test_s_len)}\")\n",
    "\n",
    "nb = 60\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(train_s_len, bins=nb, range=[0, 60], facecolor='red', label='train')\n",
    "\n",
    "plt.hist(test_s_len, bins=nb, range=[0, 60], facecolor='blue', label='test')\n",
    "plt.axvline(x=d_test.mean, color='cyan')\n",
    "\n",
    "plt.title(\"Sentence length\", size=24)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 16})\n",
    "plt.xticks([5*i for i in range(14)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_n = 20\n",
    "exclude = [\n",
    "    \"the\", \"of\", \"and\", \"to\", \"a\", \"in\", \"is\", \"i\",\n",
    "    \"that\", \"it\", \"for\", \"you\", \"was\", \"with\", \"on\",\n",
    "    \"as\", \"have\", \"but\", \"be\", \"they\"\n",
    "]\n",
    "\n",
    "for w in exclude:\n",
    "    del train_vocab[w]\n",
    "    del test_vocab[w]\n",
    "    \n",
    "Tmc = train_vocab.most_common()\n",
    "tmc = test_vocab.most_common()\n",
    "\n",
    "for i in range(_n):\n",
    "    print(f\"{Tmc[i]} -- {tmc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Less common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "Tmc = train_vocab.most_common()[:-n-1:-1]\n",
    "tmc = test_vocab.most_common()[:-n-1:-1]\n",
    "\n",
    "u = 0\n",
    "t = 10\n",
    "for w in train_vocab:\n",
    "    u += (train_vocab[w] <= t)\n",
    "print(f\"[train] {round(100.0 * u/len(train_vocab), 3)}% words have <= {t} occurences\")\n",
    "    \n",
    "u = 0\n",
    "t = 10\n",
    "for w in test_vocab:\n",
    "    u += (test_vocab[w] <= t)\n",
    "print(f\"[test]  {round(100.0 * u/len(train_vocab), 3)}% words have <= {t} occurences\")\n",
    "\n",
    "print()\n",
    "    \n",
    "for i in range(n):\n",
    "    print(f\"{Tmc[i]} -- {tmc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_in_train = Counter()\n",
    "train_not_in_test = Counter()\n",
    "\n",
    "for w in test_vocab:\n",
    "    if w not in train_vocab:\n",
    "        test_not_in_train[w] += 1\n",
    "\n",
    "for w in train_vocab:\n",
    "    if w not in test_vocab:\n",
    "        train_not_in_test[w] += 1\n",
    "        \n",
    "train_uniq_words = set(train_vocab.keys())\n",
    "test_uniq_words = set(test_vocab.keys())\n",
    "uniq_words = set(train_uniq_words.union(test_uniq_words))\n",
    "all_oov = Counter()\n",
    "\n",
    "for w in uniq_words:\n",
    "    if w not in emb_glove:\n",
    "        all_oov[w] += 1\n",
    "        \n",
    "print(f\"train not in test: {len(train_not_in_test)}\")\n",
    "print(f\"test not in train: {len(test_not_in_train)}\")\n",
    "print(f\"train uniq: {len(train_uniq_words)}\")\n",
    "print(f\"test uniq: {len(test_uniq_words)}\")\n",
    "print(f\"total uniq words: {len(uniq_words)}\")\n",
    "\n",
    "# all_oov.most_common(10)\n",
    "\",\".join([x for (x, _) in test_not_in_train.most_common(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_vocab_coverage(vocab, emb) -> (Counter, Counter):\n",
    "    oov = Counter() # out-of-vocab\n",
    "    inv = Counter() # in-vocab\n",
    "    oov_uniq_num = inv_uniq_num = 0.0\n",
    "    oov_all_num = inv_all_num = 0.0\n",
    "    \n",
    "    for w in tqdm(vocab):\n",
    "        if w in emb:\n",
    "            inv[w] = vocab[w]\n",
    "            inv_uniq_num += 1\n",
    "            inv_all_num += vocab[w]\n",
    "        else:\n",
    "            oov[w] = vocab[w]\n",
    "            oov_uniq_num += 1\n",
    "            oov_all_num += vocab[w]\n",
    "    \n",
    "    cov_uniq = 100.0 * round(inv_uniq_num / len(vocab), 5)\n",
    "    cov_all = 100.0 * round(inv_all_num / (inv_all_num + oov_all_num), 5)\n",
    "    \n",
    "    print(f\"oov_uniq: {oov_uniq_num}; inv_uniq: {inv_uniq_num}; all_uniq: {len(vocab)}\")\n",
    "    print(\"embeddings-vocabulary coverage (unique): %.3f%%\" % cov_uniq)\n",
    "    print(\"embeddings-vocabulary coverage (all text): %.3f%%\" % cov_all)\n",
    "    \n",
    "    return oov, inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adc5b167b8a4e37ac1be6f7e643bcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2de2033923546019761fce43cb12c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2196016), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2020e7eaf85b483cbdac068682d63798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1703755), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def combine_emb_dicts(*embs):\n",
    "    out_emb = defaultdict(lambda : np.zeros(300, dtype=np.float32))\n",
    "    \n",
    "    n = len(embs)\n",
    "    \n",
    "    for emb in tqdm(embs, total=n):\n",
    "        for w, e in tqdm(emb.items()):\n",
    "            out_emb[w] += (1.0/n) * e\n",
    "                \n",
    "    return out_emb\n",
    "            \n",
    "\n",
    "emb_glove_paragram = combine_emb_dicts(emb_glove, emb_paragram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae90b83e18f43b7be75ea7ec37e7142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=207081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "oov_uniq: 59460.0; inv_uniq: 147621.0; all_uniq: 207081\n",
      "embeddings-vocabulary coverage (unique): 71.287%\n",
      "embeddings-vocabulary coverage (all text): 99.545%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'uceed(123),demonetisation(115),machedo(108),gdpr(107),adityanath(106),boruto(102),upwork(101),bnbr(100),alshamsi(92),dceu(90),sjws(86),unacademy(86),iiest(86),zerodha(80),tensorflow(73),doklam(70),lnmiit(68),muoet(66),nicmar(62),vajiram(60),adhaar(59),zebpay(58),kavalireddi(58),elitmus(57),srmjee(56),reactjs(53),awdhesh(52),jiren(51),wwwyoutubecom(51),ryzen(50),root3(48),baahubali(48),koinex(46),mhcet(45),binance(44),byju(43),amazonin(42),srmjeee(42),beerus(41),sgsits(40),skripal(40),ftre(39),nanodegree(38),gurugram(38),hotstar(38),mhtcet(38),bipc(37),bmsce(37),jiofi(36),microservices(36)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov, inv = get_emb_vocab_coverage(train_vocab, emb_glove_paragram)\n",
    "\",\".join([x + f\"({y})\" for (x, y) in oov.most_common(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov, inv = get_emb_vocab_coverage(test_vocab, emb_paragram)\n",
    "\",\".join([x + f\"({y})\" for (x, y) in oov.most_common(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_thrd = [x for (x, y) in oov.most_common() if y > 50]\n",
    "\n",
    "[w for w in oov_thrd if w in emb_word2vec or w in emb_wiki]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandleMisspellings:\n",
    "\n",
    "    def __init__(self, all_words_set, words2idx):\n",
    "        self.all_words_set = all_words_set\n",
    "        self.words2idx = words2idx\n",
    "    \n",
    "    def prob(self, word):\n",
    "        return self.words2idx.get(word, 0)\n",
    "\n",
    "\n",
    "    def one_edit(self, word):\n",
    "        letters = string.ascii_lowercase\n",
    "\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "    def known(self, words):\n",
    "        return set(words).intersection(self.all_words_set)\n",
    "\n",
    "\n",
    "    def candidates(self, word):\n",
    "        return self.known([word]).union(self.known(self.one_edit(word)))\n",
    "\n",
    "\n",
    "    def correction(self, word):\n",
    "        cs = self.candidates(word)\n",
    "        return word if len(cs) == 0 else min(cs, key=lambda w: self.prob(w))\n",
    "\n",
    "misspelling_handler = HandleMisspellings(\n",
    "    all_words_set=set(list(emb_glove_paragram.keys())),\n",
    "    words2idx={w: i for (i, w) in enumerate(emb_glove_paragram.keys())}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4d8fe3f557440a9cffe4b778c2c9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59460), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8.895771503448486 0.5731752438614195\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "t = 0\n",
    "for x, _ in tqdm(oov.most_common()):\n",
    "    y = misspelling_handler.correction(x)\n",
    "    t += int(x != y)\n",
    "print(time.time() - s, t/len(oov.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # GloVe emb matrix\n",
    "        num_words, emb_size = emb_matrix.shape\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(num_words, emb_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=2*self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(2 * self.hidden_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B x sen_maxlen\n",
    "\n",
    "        emb = self.embedding(x)\n",
    "        # B x sen_maxlen x emb_size\n",
    "\n",
    "        out_lstm, _ = self.lstm(emb)\n",
    "        # B x sen_maxlen x (2*sen_maxlen)\n",
    "        \n",
    "        _, h_gru = self.gru(self.dropout(out_lstm))\n",
    "        # 2 x B x sen_maxlen\n",
    "        \n",
    "        h_gru = h_gru.permute((1, 0, 2)).reshape(x.size(0), -1)\n",
    "        # B x (2*sen_maxlen)\n",
    "        \n",
    "        out = self.fc(h_gru).unsqueeze(0)\n",
    "        # 1 x B x 1\n",
    "        \n",
    "        return out\n",
    "\n",
    "# --\n",
    "bs = 8\n",
    "x = torch.zeros((bs, 70), dtype=torch.long)\n",
    "m = Net(emb_matrix=np.load(\"glove_embedding_matrix.npy\"), hidden_size=70)\n",
    "\n",
    "y = m(x)\n",
    "\n",
    "y.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
