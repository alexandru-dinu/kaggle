{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "991dd280e1aa895006782f2420299f43c365bf3b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from functools import reduce\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.tensor as tensor\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "243278ccda7ad7c9d6d4352f7fca1d44549d1084"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input\"\n",
    "TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_CSV = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}; cols: {list(train_df.columns)}\")\n",
    "print(f\"Test shape: {test_df.shape}; cols: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6390d234bfae617a0628e8d72329f5a212d1edb"
   },
   "outputs": [],
   "source": [
    "sincere = train_df.loc[train_df['target'] == 0]\n",
    "insincere = train_df.loc[train_df['target'] == 1]\n",
    "\n",
    "print(\n",
    "    f\"sincere: {len(sincere)} ({round(100.0 * len(sincere)/len(train_df), 3)}%); \"\n",
    "    f\"insincere: {len(insincere)} ({round(100.0 * len(insincere)/len(train_df), 3)}%); \"\n",
    "    f\"ratio (-/+): {round(len(sincere)/len(insincere), 3)}; \"\n",
    "    f\"ratio (+/-): {round(len(insincere)/len(sincere), 3)}\\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"sincere: {sincere.iloc[random.randint(0, len(sincere))]['question_text']}\\n\\n\"\n",
    "    f\"insincere: {insincere.iloc[random.randint(0, len(insincere))]['question_text']}\"\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_GLOVE_FILE = f\"{DATA_DIR}/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "EMB_WORD2VEC_FILE = f\"{DATA_DIR}/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\n",
    "EMB_PARAGRAM_FILE = f\"{DATA_DIR}/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "EMB_WIKI_FILE = f\"{DATA_DIR}/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_word2vec = KeyedVectors.load_word2vec_format(EMB_WORD2VEC_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(emb_word2vec.vocab)} x {emb_word2vec['the'].size}\")\n",
    "print(\"xiaomi\" in emb_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_word2vec.similar_by_vector(emb_word2vec['parameter'], topn=20, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_WIKI_FILE) if len(o)>100)\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "emb_wiki = load_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(emb_wiki)} x {emb_wiki['the'].size}\")\n",
    "print(\"xiaomi\" in emb_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_GLOVE_FILE, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "\n",
    "emb_glove = load_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(emb_glove)} x {emb_glove['a'].size}\")\n",
    "print(\"parameter\" in emb_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_to(w, n=1):\n",
    "    xs = []\n",
    "    \n",
    "    for w_ in tqdm(emb_glove):\n",
    "        if w == w_: continue\n",
    "        xs += [(w_, np.dot(emb_glove[w], emb_glove[w_])/(np.linalg.norm(emb_glove[w]) * np.linalg.norm(emb_glove[w_])))]\n",
    "    \n",
    "    return [x for x, _ in sorted(xs, key=lambda x:-x[1])[:n]]\n",
    "\n",
    "closest_to(\"function\", n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParaGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paragram():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_PARAGRAM_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    return embeddings_index\n",
    "    \n",
    "emb_paragram = load_paragram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(emb_paragram)} x {emb_paragram['the'].size}\")\n",
    "print(\"paytm\" in emb_paragram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_emb_dicts(*embs):\n",
    "    out_emb = defaultdict(lambda : np.zeros(300, dtype=np.float32))\n",
    "    \n",
    "    n = len(embs)\n",
    "    \n",
    "    for emb in tqdm(embs, total=n):\n",
    "        for w, e in tqdm(emb.items()):\n",
    "            out_emb[w] += (1.0/n) * e\n",
    "                \n",
    "    return out_emb\n",
    "            \n",
    "\n",
    "emb_glove_paragram = combine_emb_dicts(emb_glove, emb_paragram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_glove_not_w2v = Counter()\n",
    "_w2v_not_glove = Counter()\n",
    "\n",
    "for w in tqdm(emb_word2vec.vocab):\n",
    "    if w not in emb_glove:\n",
    "        _w2v_not_glove[w] += 1\n",
    "\n",
    "for w in tqdm(emb_glove):\n",
    "    if w not in emb_word2vec:\n",
    "        _glove_not_w2v[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"glove not w2v: {len(_glove_not_w2v)}; w2v not glove: {len(_w2v_not_glove)}\")\n",
    "print(\"-\" * 64)\n",
    "print(random.sample(set(_w2v_not_glove), 10))\n",
    "print(\"-\" * 64)\n",
    "print(random.sample(set(_glove_not_w2v), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e6ac0681544ffa4ddf6af342222d80f9407fda3"
   },
   "outputs": [],
   "source": [
    "PUNCTUATION = {\n",
    "    'sep'   : u'\\u200b' + \"/-'´′‘…—−–\",\n",
    "    'keep'  : \"&\",\n",
    "    'remove': '?!.,，\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~“”’™•°'\n",
    "}\n",
    "\n",
    "SYN_DICT = {\n",
    "    'cryptocurrencies': 'crypto currencies',\n",
    "    'ethereum'        : 'crypto currency',\n",
    "    'coinbase'        : 'crypto platform',\n",
    "    'altcoin'         : 'crypto currency',\n",
    "    'altcoins'        : 'crypto currency',\n",
    "    'litecoin'        : 'crypto currency',\n",
    "    'fortnite'        : 'video game',\n",
    "    'quorans'         : 'quora members',\n",
    "    'quoras'          : 'quora members',\n",
    "    'qoura'           : 'quora',\n",
    "    'brexit'          : 'britain exit',\n",
    "    'redmi'           : 'phone',\n",
    "    'oneplus'         : 'phone',\n",
    "    'hackerrank'      : 'programming challenges',\n",
    "    'bhakts'          : 'gullible',\n",
    "    '√'               : 'square root',\n",
    "    '÷'               : 'division',\n",
    "    '∞'               : 'infinity',\n",
    "    '€'               : 'euro',\n",
    "    '£'               : 'pound sterling',\n",
    "    '$'               : 'dollar',\n",
    "    '₹'               : 'rupee',\n",
    "    '×'               : 'product',\n",
    "    'ã'               : 'a',\n",
    "    'è'               : 'e',\n",
    "    'é'               : 'e',\n",
    "    'ö'               : 'o',\n",
    "    '²'               : 'squared',\n",
    "    '∈'               : 'in',\n",
    "    '∩'               : 'intersection',\n",
    "    u'\\u0398'         : 'Theta',\n",
    "    u'\\u03A0'         : 'Pi',\n",
    "    u'\\u03A9'         : 'Omega',\n",
    "    u'\\u0392'         : 'Beta',\n",
    "    u'\\u03B8'         : 'theta',\n",
    "    u'\\u03C0'         : 'pi',\n",
    "    u'\\u03C9'         : 'omega',\n",
    "    u'\\u03B2'         : 'beta',\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return list(map(lambda w: w.strip(), s.split()))\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "\n",
    "    for p in PUNCTUATION['sep']:\n",
    "        x = x.replace(p, \" \")\n",
    "    for p in PUNCTUATION['keep']:\n",
    "        x = x.replace(p, f\" {p} \")\n",
    "    for p in PUNCTUATION['remove']:\n",
    "        x = x.replace(p, \"\")\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_site(x):\n",
    "    regex = re.compile('(www)([a-z0-9]+)(com|org)')\n",
    "    return regex.sub(lambda m: m.group(2), x)\n",
    "\n",
    "\n",
    "def clean_syn(x):\n",
    "    regex = re.compile('(%s)' % '|'.join(SYN_DICT.keys()))\n",
    "    return regex.sub(lambda m: SYN_DICT.get(m.group(0), ''), x)\n",
    "\n",
    "\n",
    "def clean_all(x):\n",
    "    x = clean_text(x)\n",
    "    x = clean_syn(x)\n",
    "    x = clean_site(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def build_vocabulary(df: pd.DataFrame) -> Counter:\n",
    "    sentences = df.progress_apply(tokenize).values\n",
    "    vocab = Counter()\n",
    "    s_len = []\n",
    "    \n",
    "    for sentence in tqdm(sentences):  \n",
    "        s_len.append(len(sentence))\n",
    "        for word in sentence:\n",
    "            vocab[word] += 1\n",
    "    return vocab, np.array(s_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "train_df[\"clean_question_text\"] = train_df[\"question_text\"].progress_apply(clean_all)\n",
    "test_df[\"clean_question_text\"] = test_df[\"question_text\"].progress_apply(clean_all)\n",
    "\n",
    "# vocab\n",
    "train_vocab, train_s_len = build_vocabulary(train_df[\"clean_question_text\"])\n",
    "test_vocab, test_s_len = build_vocabulary(test_df[\"clean_question_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = scipy.stats.describe(train_s_len)\n",
    "d_test = scipy.stats.describe(test_s_len)\n",
    "print(f\"train: {d_train}, median: {np.median(train_s_len)}\")\n",
    "print(f\"test: {d_test}, median: {np.median(test_s_len)}\")\n",
    "\n",
    "nb = 60\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(train_s_len, bins=nb, range=[0, 60], facecolor='red', label='train')\n",
    "\n",
    "plt.hist(test_s_len, bins=nb, range=[0, 60], facecolor='blue', label='test')\n",
    "plt.axvline(x=d_test.mean, color='cyan')\n",
    "\n",
    "plt.title(\"Sentence length\", size=24)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 16})\n",
    "plt.xticks([5*i for i in range(14)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_n = 20\n",
    "exclude = [\n",
    "    \"the\", \"of\", \"and\", \"to\", \"a\", \"in\", \"is\", \"i\",\n",
    "    \"that\", \"it\", \"for\", \"you\", \"was\", \"with\", \"on\",\n",
    "    \"as\", \"have\", \"but\", \"be\", \"they\"\n",
    "]\n",
    "\n",
    "for w in exclude:\n",
    "    del train_vocab[w]\n",
    "    del test_vocab[w]\n",
    "    \n",
    "Tmc = train_vocab.most_common()\n",
    "tmc = test_vocab.most_common()\n",
    "\n",
    "for i in range(_n):\n",
    "    print(f\"{Tmc[i]} -- {tmc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Less common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "Tmc = train_vocab.most_common()[:-n-1:-1]\n",
    "tmc = test_vocab.most_common()[:-n-1:-1]\n",
    "\n",
    "u = 0\n",
    "t = 10\n",
    "for w in train_vocab:\n",
    "    u += (train_vocab[w] <= t)\n",
    "print(f\"[train] {round(100.0 * u/len(train_vocab), 3)}% words have <= {t} occurences\")\n",
    "    \n",
    "u = 0\n",
    "t = 10\n",
    "for w in test_vocab:\n",
    "    u += (test_vocab[w] <= t)\n",
    "print(f\"[test]  {round(100.0 * u/len(train_vocab), 3)}% words have <= {t} occurences\")\n",
    "\n",
    "print()\n",
    "    \n",
    "for i in range(n):\n",
    "    print(f\"{Tmc[i]} -- {tmc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_in_train = Counter()\n",
    "train_not_in_test = Counter()\n",
    "\n",
    "for w in test_vocab:\n",
    "    if w not in train_vocab:\n",
    "        test_not_in_train[w] += 1\n",
    "\n",
    "for w in train_vocab:\n",
    "    if w not in test_vocab:\n",
    "        train_not_in_test[w] += 1\n",
    "        \n",
    "train_uniq_words = set(train_vocab.keys())\n",
    "test_uniq_words = set(test_vocab.keys())\n",
    "uniq_words = set(train_uniq_words.union(test_uniq_words))\n",
    "all_oov = Counter()\n",
    "\n",
    "for w in uniq_words:\n",
    "    if w not in emb_glove:\n",
    "        all_oov[w] += 1\n",
    "        \n",
    "print(f\"train not in test: {len(train_not_in_test)}\")\n",
    "print(f\"test not in train: {len(test_not_in_train)}\")\n",
    "print(f\"train uniq: {len(train_uniq_words)}\")\n",
    "print(f\"test uniq: {len(test_uniq_words)}\")\n",
    "print(f\"total uniq words: {len(uniq_words)}\")\n",
    "\n",
    "# all_oov.most_common(10)\n",
    "\",\".join([x for (x, _) in test_not_in_train.most_common(50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandleMisspellings:\n",
    "\n",
    "    def __init__(self, all_words_set, words2idx):\n",
    "        self.all_words_set = all_words_set\n",
    "        self.words2idx = words2idx\n",
    "\n",
    "    def prob(self, word):\n",
    "        return self.words2idx.get(word, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def one_edit(word):\n",
    "        letters = string.ascii_lowercase\n",
    "\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def known(self, words):\n",
    "        return set(words).intersection(self.all_words_set)\n",
    "\n",
    "    def candidates(self, word):\n",
    "        return self.known([word]).union(self.known(self.one_edit(word)))\n",
    "\n",
    "    def correct(self, word):\n",
    "        cs = self.candidates(word)\n",
    "        return word if len(cs) == 0 else min(cs, key=lambda w: self.prob(w))\n",
    "\n",
    "misspelling_handler = HandleMisspellings(\n",
    "    all_words_set=set(list(emb_glove_paragram.keys())),\n",
    "    words2idx={w: i for (i, w) in enumerate(emb_glove_paragram.keys())}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "t = 0\n",
    "misspelling_handler.correct('dang3r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embbedding coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_vocab_coverage(vocab, emb) -> (Counter, Counter):\n",
    "    oov = Counter() # out-of-vocab\n",
    "    inv = Counter() # in-vocab\n",
    "    oov_uniq_num = inv_uniq_num = 0.0\n",
    "    oov_all_num = inv_all_num = 0.0\n",
    "    \n",
    "    for w in tqdm(vocab):\n",
    "        if w in emb or misspelling_handler.correction(w) in emb:\n",
    "            inv[w] = vocab[w]\n",
    "            inv_uniq_num += 1\n",
    "            inv_all_num += vocab[w]\n",
    "        else:\n",
    "            oov[w] = vocab[w]\n",
    "            oov_uniq_num += 1\n",
    "            oov_all_num += vocab[w]\n",
    "    \n",
    "    cov_uniq = 100.0 * round(inv_uniq_num / len(vocab), 5)\n",
    "    cov_all = 100.0 * round(inv_all_num / (inv_all_num + oov_all_num), 5)\n",
    "    \n",
    "    print(f\"oov_uniq: {oov_uniq_num}; inv_uniq: {inv_uniq_num}; vocab_size: {len(vocab)}\")\n",
    "    print(\"embeddings-vocabulary coverage (unique): %.3f%%\" % cov_uniq)\n",
    "    print(\"embeddings-vocabulary coverage (all text): %.3f%%\" % cov_all)\n",
    "    \n",
    "    return oov, inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov, inv = get_emb_vocab_coverage(train_vocab, emb_glove_paragram)\n",
    "\",\".join([x + f\"({y})\" for (x, y) in oov.most_common(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov, inv = get_emb_vocab_coverage(test_vocab, emb_paragram)\n",
    "\",\".join([x + f\"({y})\" for (x, y) in oov.most_common(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_thrd = [x for (x, y) in oov.most_common() if y > 0]\n",
    "len([w for w in oov_thrd if w in emb_wiki])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, with_bias=False):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.with_bias = with_bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight, requires_grad=True)\n",
    "\n",
    "        if with_bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(step_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), # (B * step_dim) x feature_dim\n",
    "            self.weight                           # feature_dim x 1\n",
    "        ).view(-1, step_dim)\n",
    "\n",
    "        if self.with_bias:\n",
    "            eij = eij + self.bias\n",
    "\n",
    "        eij = torch.tanh(eij)\n",
    "        # B x step_dim\n",
    "\n",
    "        a = torch.exp(eij)\n",
    "        a = a / (torch.sum(a, dim=1, keepdim=True) + 1e-10)\n",
    "        # B x step_dim\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        # B x step_dim x feature_dim\n",
    "        \n",
    "        # sum over step_dim\n",
    "        return torch.sum(weighted_input, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Attention(2*70, 70, True)\n",
    "x = torch.zeros((5, 70, 2*70))\n",
    "y = a(x)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        num_words, emb_size = emb_matrix.shape\n",
    "\n",
    "        # sentence maxlen\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(num_words, emb_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.bidir_lstm1 = nn.LSTM(\n",
    "            input_size=emb_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.lstm1_attention = Attention(\n",
    "            feature_dim=2 * self.hidden_size, step_dim=self.hidden_size, with_bias=True\n",
    "        )\n",
    "\n",
    "        self.bidir_lstm2 = nn.LSTM(\n",
    "            input_size=2 * self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.lstm2_attention = Attention(\n",
    "            feature_dim=2 * self.hidden_size, step_dim=self.hidden_size, with_bias=True\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(4 * 2 * self.hidden_size, 2 * self.hidden_size)\n",
    "        self.fc2 = nn.Linear(2 * self.hidden_size, 1)\n",
    "        \n",
    "        nn.init.orthogonal_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "\n",
    "        self.dropout_emb = nn.Dropout2d(0.1)\n",
    "        self.dropout_rnn = nn.Dropout(0.4)\n",
    "        self.dropout_fc = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B x sen_maxlen\n",
    "\n",
    "        emb = self.dropout_emb(self.embedding(x))\n",
    "        # B x sen_maxlen x emb_size\n",
    "\n",
    "        out_lstm1, _ = self.bidir_lstm1(emb)\n",
    "        # B x sen_maxlen x (2*sen_maxlen)\n",
    "\n",
    "        out_lstm1_atn = self.lstm1_attention(out_lstm1)\n",
    "        # B x (2*sen_maxlen)\n",
    "\n",
    "        out_lstm2, _ = self.bidir_lstm2(self.dropout_rnn(out_lstm1))\n",
    "        # B x sen_maxlen x (2*sen_maxlen)\n",
    "\n",
    "        out_lstm2_atn = self.lstm2_attention(out_lstm2)\n",
    "        # B x (2*sen_maxlen)\n",
    "\n",
    "        # pooling\n",
    "        max_pool, _ = torch.max(out_lstm2, dim=1)\n",
    "        # B x (2*sen_maxlen)\n",
    "        avg_pool = torch.mean(out_lstm2, dim=1)\n",
    "        # B x (2*sen_maxlen)\n",
    "\n",
    "        # concatenate results\n",
    "        out = torch.cat((out_lstm1_atn, out_lstm2_atn, max_pool, avg_pool), dim=1)\n",
    "        # B x (4 * 2*sen_maxlen)\n",
    "\n",
    "        out = self.fc2(self.dropout_fc(self.relu(self.fc1(out)))).unsqueeze(0)\n",
    "        # 1 x B x 1\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 5\n",
    "x = torch.zeros((bs, 70), dtype=torch.long)\n",
    "m = Net(emb_matrix=np.zeros((1000,300)), hidden_size=70)\n",
    "\n",
    "y = m(x)\n",
    "\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1 = test_df[['qid']].copy()\n",
    "submission1.head()\n",
    "\n",
    "submission2 = pd.read_csv('../input/sample_submission.csv')\n",
    "submission2.head()\n",
    "\n",
    "all(submission1[['qid']] == submission2[['qid']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
