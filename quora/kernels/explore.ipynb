{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "991dd280e1aa895006782f2420299f43c365bf3b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.tensor as tensor\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "243278ccda7ad7c9d6d4352f7fca1d44549d1084"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input\"\n",
    "TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_CSV = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}; cols: {list(train_df.columns)}\")\n",
    "print(f\"Test shape: {test_df.shape}; cols: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6390d234bfae617a0628e8d72329f5a212d1edb"
   },
   "outputs": [],
   "source": [
    "sincere = train_df.loc[train_df['target'] == 0]\n",
    "insincere = train_df.loc[train_df['target'] == 1]\n",
    "\n",
    "print(\n",
    "    f\"sincere: {len(sincere)} ({round(100.0 * len(sincere)/len(train_df), 3)}%); \"\n",
    "    f\"insincere: {len(insincere)} ({round(100.0 * len(insincere)/len(train_df), 3)}%)\\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"sincere: {sincere.iloc[random.randint(0, len(sincere))]['question_text']}\\n\\n\"\n",
    "    f\"insincere: {insincere.iloc[random.randint(0, len(insincere))]['question_text']}\"\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_GLOVE = f\"{DATA_DIR}/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "EMB_WORD2VEC = f\"{DATA_DIR}/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\n",
    "EMB_PARAGRAM = f\"{DATA_DIR}/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "EMB_WIKI = f\"{DATA_DIR}/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_word2vec = KeyedVectors.load_word2vec_format(EMB_WORD2VEC, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(emb_word2vec.vocab)\n",
    "# print(\"barbiturates\" in emb_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove():\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMB_GLOVE, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "\n",
    "emb_glove = load_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(emb_glove)} x {emb_glove['a'].size}\")\n",
    "print(\"xiaomi\" in emb_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e6ac0681544ffa4ddf6af342222d80f9407fda3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEP_PUNCTS         = u'\\u200b' + \"/-'´‘…—−–\"\n",
    "SHOULD_KEEP_PUNCTS = \"&\"\n",
    "TO_REMOVE_PUNCTS   = '?!.,，\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~“”’™•°'\n",
    "\n",
    "GLOVE_SYN_DICT = {\n",
    "    'cryptocurrencies': 'crypto currencies',\n",
    "    'ethereum'        : 'crypto currency',\n",
    "    'fortnite'        : 'video game',\n",
    "    'quorans'         : 'quora members',\n",
    "    'brexit'          : 'britain exit',\n",
    "    'redmi'           : 'xiaomi',\n",
    "    '√'               : 'square root',\n",
    "    '÷'               : 'division',\n",
    "    '∞'               : 'infinity',\n",
    "    '€'               : 'euro',\n",
    "    '£'               : 'pound sterling',\n",
    "    '$'               : 'dollar',\n",
    "    '₹'               : 'rupee',\n",
    "    '×'               : 'product',\n",
    "    'ã'               : 'a',\n",
    "    'è'               : 'e',\n",
    "    'é'               : 'e',\n",
    "    'ö'               : 'o',\n",
    "    '²'               : 'squared',\n",
    "    '∈'               : 'in',\n",
    "    '∩'               : 'intersection',\n",
    "    u'\\u0398'         : 'Theta',\n",
    "    u'\\u03A0'         : 'Pi',\n",
    "    u'\\u03A9'         : 'Omega',\n",
    "    u'\\u0392'         : 'Beta',\n",
    "    u'\\u03B8'         : 'theta',\n",
    "    u'\\u03C0'         : 'pi',\n",
    "    u'\\u03C9'         : 'omega',\n",
    "    u'\\u03B2'         : 'beta',\n",
    "}\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return list(map(lambda w : w.strip(), s.split()))\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "    \n",
    "    for p in SEP_PUNCTS:\n",
    "        x = x.replace(p, \" \")\n",
    "    for p in SHOULD_KEEP_PUNCTS:\n",
    "        x = x.replace(p, f\" {p} \")\n",
    "    for p in TO_REMOVE_PUNCTS:\n",
    "        x = x.replace(p, \"\")\n",
    "    \n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def clean_syn(x):\n",
    "    regex = re.compile('(%s)' % '|'.join(GLOVE_SYN_DICT.keys()))\n",
    "    return regex.sub(lambda m : GLOVE_SYN_DICT.get(m.group(0), ''), x)\n",
    "\n",
    "def clean_all(x):\n",
    "    x = clean_text(x)\n",
    "    x = clean_syn(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "def build_vocabulary(df: pd.DataFrame) -> Counter:\n",
    "    sentences = df.progress_apply(tokenize).values\n",
    "    vocab = Counter()\n",
    "    s_len = []\n",
    "    \n",
    "    for sentence in tqdm(sentences):  \n",
    "        s_len.append(len(sentence))\n",
    "        for word in sentence:\n",
    "            vocab[word] += 1\n",
    "    return vocab, np.array(s_len)\n",
    "\n",
    "\n",
    "# clean\n",
    "train_df[\"clean_question_text\"] = train_df[\"question_text\"].progress_apply(clean_all)\n",
    "test_df[\"clean_question_text\"] = test_df[\"question_text\"].progress_apply(clean_all)\n",
    "\n",
    "# vocab\n",
    "train_vocab, train_s_len = build_vocabulary(train_df[\"clean_question_text\"])\n",
    "test_vocab, test_s_len = build_vocabulary(test_df[\"clean_question_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = scipy.stats.describe(train_s_len)\n",
    "d_test = scipy.stats.describe(test_s_len)\n",
    "print(f\"train: {d_train}\")\n",
    "print(f\"test: {d_test}\")\n",
    "\n",
    "nb = 60\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(train_s_len, bins=nb, range=[0, 60], facecolor='red', label='train')\n",
    "\n",
    "plt.hist(test_s_len, bins=nb, range=[0, 60], facecolor='blue', label='test')\n",
    "plt.axvline(x=d_test.mean, color='cyan')\n",
    "\n",
    "plt.title(\"Sentence length\", size=24)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 16})\n",
    "plt.xticks([5*i for i in range(14)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_n = 10\n",
    "print(train_vocab.most_common(_n))\n",
    "print(train_vocab.most_common()[:-_n-1:-1])\n",
    "print(\"-\"*64)\n",
    "print(test_vocab.most_common(_n))\n",
    "print(test_vocab.most_common()[:-_n-1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_vocab_coverage(vocab, emb) -> (Counter, Counter):\n",
    "    oov = Counter() # out-of-vocab\n",
    "    inv = Counter() # in-vocab\n",
    "    oov_uniq_num = inv_uniq_num = 0.0\n",
    "    oov_all_num = inv_all_num = 0.0\n",
    "    \n",
    "    for w in tqdm(vocab):\n",
    "        if w in emb:\n",
    "            inv[w] = vocab[w]\n",
    "            inv_uniq_num += 1\n",
    "            inv_all_num += vocab[w]\n",
    "        else:\n",
    "            oov[w] = vocab[w]\n",
    "            oov_uniq_num += 1\n",
    "            oov_all_num += vocab[w]\n",
    "    \n",
    "    cov_uniq = 100.0 * round(inv_uniq_num / len(vocab), 5)\n",
    "    cov_all = 100.0 * round(inv_all_num / (inv_all_num + oov_all_num), 5)\n",
    "    \n",
    "    print(f\"oov_uniq: {oov_uniq_num}; inv_uniq: {inv_uniq_num}; all_uniq: {len(vocab)}\")\n",
    "    print(\"embeddings-vocabulary coverage (unique): %.3f%%\" % cov_uniq)\n",
    "    print(\"embeddings-vocabulary coverage (all text): %.3f%%\" % cov_all)\n",
    "    \n",
    "    return oov, inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov, inv = get_emb_vocab_coverage(train_vocab, emb_glove)\n",
    "oov.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov, inv = get_emb_vocab_coverage(test_vocab, emb_glove)\n",
    "oov.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(df, len_voc, sentence_maxlen):\n",
    "    t = Tokenizer(num_words=len_voc, filters='')\n",
    "    t.fit_on_texts(df['clean_question_text'])\n",
    "    \n",
    "    X = pad_sequences(\n",
    "        t.texts_to_sequences(df['clean_question_text']), \n",
    "        maxlen=sentence_maxlen\n",
    "    )\n",
    "    Y = df['target'].values\n",
    "    \n",
    "    return X, Y, t.word_index\n",
    "\n",
    "# X = (train_size x sentence_maxlen)\n",
    "X_train, Y_train, word_index = make_data(\n",
    "    train_df, len_voc=len(train_vocab), sentence_maxlen=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(embeddings_dict, word_index, len_voc):\n",
    "    \"\"\"\n",
    "    Random values of oov words\n",
    "    \"\"\"\n",
    "    all_embs = np.stack(list(embeddings_dict.values()))\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    \n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "    \n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= len_voc:\n",
    "            continue\n",
    "        embedding_vector = embeddings_dict.get(word, None)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = make_embedding_matrix(emb_glove, word_index, len_voc=len(train_vocab))\n",
    "print(all(embedding_matrix[word_index['test'], :] == emb_glove['test']))\n",
    "\n",
    "del word_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
