{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "991dd280e1aa895006782f2420299f43c365bf3b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "243278ccda7ad7c9d6d4352f7fca1d44549d1084"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input\"\n",
    "TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_CSV = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}; cols: {list(train_df.columns)}\")\n",
    "print(f\"Test shape: {test_df.shape}; cols: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6390d234bfae617a0628e8d72329f5a212d1edb"
   },
   "outputs": [],
   "source": [
    "# randomly show a train example\n",
    "list(train_df.iloc[random.randint(0, len(train_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48db903b91a46aebc6ed9996499ebe3390ac45f3"
   },
   "outputs": [],
   "source": [
    "sincere = train_df.loc[train_df['target'] == 0]\n",
    "insincere = train_df.loc[train_df['target'] == 1]\n",
    "\n",
    "print(insincere.iloc[random.randint(0, len(insincere))]['question_text'])\n",
    "\n",
    "print(f\"Sincere: {len(sincere)} ({round(100.0 * len(sincere)/len(train_df), 3)}%)\")\n",
    "print(f\"Insincere: {len(insincere)} ({round(100.0 * len(insincere)/len(train_df), 3)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e6ac0681544ffa4ddf6af342222d80f9407fda3"
   },
   "outputs": [],
   "source": [
    "def tokenize(s: str):\n",
    "    xs = list(map(lambda w : w.lower().split(\" \"), re.findall(r\"[\\w']+\", s)))\n",
    "    xs = set(reduce(lambda x, y : x + y, xs, []))\n",
    "    \n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f08bb30b7812885d0411eb752085abbd5740a8a"
   },
   "outputs": [],
   "source": [
    "def train(df):\n",
    "    # P_sincere[word] = p(sincere | word)\n",
    "    p_sincere = Counter()\n",
    "    # P_insincere[word] = p(insincere | word)\n",
    "    p_insincere = Counter()\n",
    "\n",
    "    for (_, question, t) in tqdm(train_df[['question_text', 'target']].itertuples(), total=len(train_df)):\n",
    "        for w in tokenize(question):            \n",
    "            p_sincere[w]   += (t == 0) * 1\n",
    "            p_insincere[w] += (t == 1) * 1\n",
    "    \n",
    "    return p_sincere, p_insincere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3785083fe8fcc1d17d9a9b05178d59ea6a12c934"
   },
   "outputs": [],
   "source": [
    "p_sincere, p_insincere = train(train_df)\n",
    "num_sincere_words = sum(p_sincere.values())\n",
    "num_insincere_words = sum(p_insincere.values())\n",
    "voc_len = len(set(p_sincere.keys()).union(set(p_insincere.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68d28bb2d64a73f35a2c3d6428741a829f90ae20"
   },
   "outputs": [],
   "source": [
    "def predict(question, alpha=1):\n",
    "    log_s, log_i = np.log(0.5), np.log(0.5)\n",
    "    \n",
    "    s_factor = 1.0 / (num_sincere_words + voc_len * alpha)\n",
    "    i_factor = 1.0 / (num_insincere_words + voc_len * alpha)\n",
    "    \n",
    "    for w in tokenize(question):\n",
    "        if w in p_sincere:\n",
    "            log_s += np.log(s_factor * (p_sincere[w] + alpha))\n",
    "            \n",
    "        if w in p_insincere:\n",
    "            log_i += np.log(i_factor * (p_insincere[w] + alpha))\n",
    "        \n",
    "    return (1, log_i) if log_i >= log_s else (0, log_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Why do Americans have an average IQ of 78 wheres I have an IQ of 159?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6368dc2a277cc3719e3199eff742b6f06e0456be"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for (_, qid, qtext) in tqdm(test_df.itertuples(), total=len(test_df)):\n",
    "    p, _ = predict(qtext, alpha=1)\n",
    "    preds.append(p)\n",
    "\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'qid': test_df['qid'],\n",
    "    'prediction': preds\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
